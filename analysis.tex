%% This section analyze results of the benchmark and highlights the most interesting findings. %% are presented in Section~\ref{sec:}.
Figures~\ref{fig:socapps_plots} and~\ref{fig:cfdapps_plots} summarize major benchmarking results
for social science and CFD applications respectively.
For each application, we include two plots:
the first one -- scalability plot -- illustrates the change
in the total ellapsed time with the grows of the number of MPI processes for all testbeds,
whereas the second one -- metrics plot -- present all metrics measured for Intel Xeon Gold 6140 with different number of MPI processes.
Intel Xeon Gold 6140 has been  chosen to report details on measured metrics 
since it demonstrated the best performance compared to other testbeds (see Section\,\ref{sec:summary}).
Besides plots for Intel Xeon Gold 6140,
reports \cite{2017:coegss_benchmark1} and \cite{2018:coegss_benchmark2} contain similar plots and supplementary information for the remaining testbeds.
Both Figures~\ref{fig:socapps_plots} and~\ref{fig:cfdapps_plots} share identical legends.
Note that scalability plot for HWRF application (Figure~\ref{fig:hwrf_scalability}) lacks information about ARM Hi1616 and IBM Power8+.
We failed to port HWRF on those architectures.

\subsection{HPC-compliant social simulation software}

%% The group of benchmarked social simulation software covers applications for pre-processingand simulation of agent-based models.
%% The benchmarks have shown that conversion from shape files to rasters is very demanding in terms of RAM while I/O and CPU requirements are rather low.
%% In order to study computational requirements to applications that generate synthetic populations,
%% we implemented a simple parallel version of the  celebrated  iterative  proportional  fitting  (IPF)  method.

Our benchmarks demonstrate high performance of IPF on different  architectures.
The application scales to a number of available cores on all testbeds in out study (see Figure~\ref{fig:ipf_scalability}).
On the reference testbed, we observe the scalability bound for more than 1400 cores.
Neither  RAM, nor I/O of modern architectures are limiting factors for IPF performance (see Figure~\ref{fig:ipf_intel_gold}).
The reason of such good results is in a heavy use of highly optimized \textsf{ScaLAPACK} kernels in the IPF implementation.
In contrast to other applications, in case of IPF, the least elapsed time is observed for AMD Epic 7551 testbed (see Figure~\ref{fig:ipf_scalability}).

Along  with IPF,
we benchmarked a green growth agent-based model (ABM) of diffusion implementated in \textsf{ABM4Py} and \textsf{Pandora} frameworks.
%% distributed ABMS frameworks following two different parallelization strategies.
%% The first framework -- \textsf{ABM4Py} -- implements the graph-based parallelization approach,
%% while the second framework  -- \textsf{Pandora} --
%% parallelizes the simulation processvia splitting of rasters on even pieces and distributing them between MPI processes.
Despite strong difference in parallelization  strategies,
in  both  cases,  we  observe  the same pattern:
ABMS  applications produce  a  big  amount  of  output  which  has  a  strong  negative  impact  on  application performance
(see Figures~\ref{fig:abms_intel_gold} and~\ref{fig:pandora_intel_gold}).
As a consequence, according to our green growth ABM, being I/O bound,
current ABMS frameworks for HPC have moderate requirements to CPU performance.
Nevertheless, we must emphasize that the results can look differently for
more complex models with sophisticated agent activities
and for simpler models which can be reduced to iterative applying of sparse matrix-vector or matrix-matrix operations
(e.g., random surfer model and PageRank).
Thus, our benchmarks for ABMS frameworks are not very illuminative
and must  be  extended  with  more sophisticated and more simple models to provide more evidences and draw stronger conclusions.
But discussion of the new representative ABMS models for benchmarking goes beyond the scope of this text.

\include{socapps_plots}
\include{cfdapps_plots}

%% The group of benchmarked social simulation software covers applications for pre-processing and simulation of agent-based models.
%% In order to study computational requirements to applications that generate synthetic populations,
%% a simple parallel version of the celebrated iterative proportional fitting (IPF) method was implemented.
%% This implementation heavily uses dense linear algebra kernels provided by a highly optimised ScaLAPACK library.
%% The benchmarks demonstrate high performance of IPF on different architectures.
%% Neither RAM, nor I/O of modern architectures are limiting factors for IPF performance.
%% Along with ABMS pre-processing applications,
%% a simple agent-based model of diffusion with the help of distributed ABMS frameworks
%% following two different parallelization strategies for ABMs with raster inputs was benchmarked.
%% The first framework (\textsf{Pandora}) written in C++ parallelises
%% the simulation process via splitting of rasters on even pieces and distributing them between MPI processes,
%% while the second framework (\textsf{ABM4Py}) is a Python code,
%% which implements the graph-based parallelization approach.
%% Despite strong difference in parallelization strategies, in both cases, the observation delivered the same pattern:
%% ABMS applications produce a big amount of output which has a strong negative impact on application performance.
%% As a consequence, according to the "toy" ABMs, being I/O bound, current ABMS frameworks for HPC have moderate requirements to CPU performance. Nevertheless, it must be emphasised that the results can look differently for complex models with sophisticated agent activities and models which can be reduced to iterations with sparse-matrix dense-vector operations, thus, the benchmarks for ABMS frameworks are not very illuminative and must be extended with more sophisticated models to draw stronger conclusions. But discussion of the new ABMS models for benchmarking goes beyond the scope of this text. Table~\ref{tab:bottlenecks_hardware} shortly summarises information about scalability of the benchmarked social simulation software and hardware bottlenecks.

\subsection{Large-scale CFD applications}

%% The group of benchmarked CFD applications includes large scale tools that simulate GSS-related scenarios like natural disasters (hurricanes, earthquakes), spread of air pollution and weather prediction. More specifically, the following open-source CFD codes have been selected the benchmarking purposes: HRWF – a parallel implementation of the hurricane weather research and forecasting (HWRF); OpenSWPC – an integrated parallel simulation code for modelling seismic wave propagation in 3D heterogeneous viscoelastic media; CMAQ – a community multiscale air quality modelling system, which combines CFD codes for conducting large scale air quality model simulations.

%% As expected, the benchmarks confirm that CFD applications are in general CPU-bound (e.g. Figure~\ref{fig:openswpc_scalability}) in contrast to the social simulation software. At the same, time output makes a solid contribution to the total elapsed time for such applications like CCTM and CM1 (e.g. Figure~\ref{fig:cm1_intel_gold}) which, in turn, imposed additional performance constraints on architectures with poor I/O speed. Nevertheless, it was observed that at some architectures memory was also a bottleneck for some choices of the number of MPI processes.
%% In addition, it was noticed that OpenSWPC is memory bound for the small number of MPI processes.
%% The relevant information about scalability of the benchmarked CFD applications and hardware bottlenecks is outlined it Table~\ref{tab:bottlenecks_hardware}.

Our measurements demonstrate that  CFD  applications  are  in  general  CPU-bound (see Figure~\ref{fig:cfdapps_plots}).
Nevertheless,   we   observed   that   at   some architectures,
memory  is also  a  bottleneck  for  some  specific choices  of the  number  of  MPI processes.
In  particular,  we  noticed  that  OpenSWPC is  memory  bound  for  the  small  number  of  MPI  processes
and CPU bound for a large number of utilized cored,
as the memory consumption monotonically decreases with the number of MPI processes in this application
(see Figure~\ref{fig:openswpc_intel_gold}).
All benchmarked applications except HWRF demonstrate the same monotonic decrease in memory consumption (see Figure~\ref{fig:cfdapps_plots}).
At the same time, system files outputs make a solid contribution to the total elapsed time for such applications
like CCTM and CM1 (see Figures~\ref{fig:cmaq_intel_gold} and~\ref{fig:cm1_intel_gold})
which, in turn, imposes additional performance constraints on architectures with low I/O bandwidth.
%% In  particular,  CMAQ (Community  Multiscale  Air  Quality) and  CM1  applications produce a lot of outputs,
%% which imposed additional performance constraints on architectures with  poor  I/O  speed.
%% We  outlined relevant  information  about  scalability  of  the  benchmarked  CFD  applications  andhardware bottlenecks in Table 3.

Intel Xeon Gold 6140 provides the least elapsed time for all CFD applications from our benchmark
with one minor exception  (see Figure~\ref{fig:cfdapps_plots}).
AMD Epic 7551 beats Intel Xeon Gold 6140 in OpenSWPC for the small number of cores (see Figure~\ref{fig:openswpc_scalability}).
Nevertheless, the performance AMD Epic 7551 degrade quickly compared to Intel Xeon Gold 6140 if the number of utilized cores grows.


\subsection{Implications for the hardware/software co-design}

Table~\ref{tab:bottlenecks_hardware} shortly summarizes information about scalability of the benchmarked social simulation software
and about hardware bottlenecks revealed in the previous subsection.
In this table, rows from group ``Bottlenecks'' reflect which of the following -- CPU, memory consumption, system files inputs or system files output --
appeared to be limiting factors for performance of the benchmarked applications,
whereas row ``Scalability'' presents maximum number of utilized CPU cores
where we observed decrease in the total elapsed time on the reference testbed -- Eagle cluster.

\begin{table}[htbp]
  \begin{minipage}{1\textwidth}
    \caption{Bottlenecks in the hardware and scalability of the benchmarked applications}
    \label{tab:bottlenecks_hardware}
  \end{minipage}
%% \begin{adjustbox}{width=1\textwidth}
%% \small
\begin{tabular}{cl|c|c|c|c|c|c|c|c|}
  \cline{3-10}
  &  & \multicolumn{4}{c|}{\bf Social simulation software} & \multicolumn{4}{c|}{\bf CFD software} \\ \cline{3-10} 
  &  & \multirow{2}{*}{\small\bf IPF} & \multicolumn{2}{c|}{\small\bf\textsf{Pandora}} & \multicolumn{1}{l|}{\small\bf\textsf{ABM4Py}}
  & \multirow{2}{*}{\small\bf HWRF} & \multirow{2}{*}{\small\bf OpenSWPC} & {\small\bf CMAQ} & \multirow{2}{*}{{\small\bf CM1}}\\ \cline{4-6} 
  &  &  & \multicolumn{1}{l|}{\footnotesize\bf Europe} & \multicolumn{1}{l|}{\footnotesize\bf World} & \multicolumn{1}{l|}{\footnotesize\bf 128x128} & & & {\small\bf CCTM} & \\ \hline
  \multicolumn{1}{|c|}{{\multirow{4}{*}{\rotatebox{90}{\bf Bottlenecks}}}} & {\bf CPU}
  & \checkmark &  &  & & \checkmark & \checkmark & \checkmark & \checkmark \\ \cline{2-10}
  \multicolumn{1}{|c|}{} & {\bf RAM}     &  &  &  &  &            & \checkmark &  & \\ \cline{2-10} 
  \multicolumn{1}{|c|}{} & {\bf Inputs}  &  &  &  &  & \checkmark &            &  & \\ \cline{2-10}
  \multicolumn{1}{|c|}{} & {\bf Outputs} &  & \checkmark & \checkmark & \checkmark  &  &  & \checkmark & \checkmark \\ \hline
  \multicolumn{2}{|c|}{\bf Scalability$^*$}
  & $\ge$1400 &  $\approx$128 & $\approx$700 & $\approx$64 & $\ge$128 & $\ge$128 & $\ge$128 & $\ge$128 \\ \hline
\end{tabular}
%\end{adjustbox}
\newline
\raggedright{\footnotesize{
    $^*$ maximum number of utilized cores of Xeon\,E5\,2682\,v4 cluster that leads to reduction of the total elapsed time.
}}
\end{table}

As Table~\ref{tab:bottlenecks_hardware} illustrates, most of the distributed GSS applications are memory bound.
Even large-scale CFD codes can be bound by I/O and RAM under special circumstances.
It allows us to  conclude  that  the  fast  memory  is  an  essential  requirement  to  HPC  clusters  for  GSS applications
whereas high CPU clock frequency plays a less important role.
Moreover, since many state-of-the-art GSS applications deal with large input and output files,
we believe that GSS  software  developers  should  invest  more  time  into  designing  file-avoiding  applications.
Our scalability tests show that hyperthreading provides little performance improvements for most  of  the  GSS  applications.
Therefore,  it  makes  little  sense  to  invest  money  in  expensive massively  multithreaded  chips  (like  Power8)  for  GSS  users.
We  also  recommend  to avoid clusters  with  GPU  accelerated  nodes
since  only  few  popular  GSS  applications  benefit  from GPUs.
In  particular,  among  widely  used  general-purpose  ABMS  frameworks  and  problem-specific  ABMS  codes  for  HPCs,
only the FLAME-GPU  (Flexible Larg-scale Agent Modelling Environment) framework  utilizes  GPUs \cite{2011:flame_gpu,2018:flame_gpu}.
Weak  use of GPUs  is  also  partially  related  to  the  fact
that  most  social  science  applications  are  memory bound.
Being more specific, among the architectures used in benchmarking,
we recommend to build clusters upon ARM Hi1616 if energy efficiency is a crucial requirement,
or upon Intel Xeon Gold 6140 if performance is a first priority while relatively high operating expense and capital expenditure are not an issue.

According  to  our  benchmarks,  the  scalability  of  GSS  applications  is  rather  diverse.
All applications from the social simulation software stack demonstrate poor scalability with one notable exception -- the IPF implementation.
Moreover, even though our benchmarks do not demonstrate this explicitly,
it is also known that social simulation software scales worse than the large-scale CFD  codes.
On  the  other  hand,  due  to  stochastic  nature  of  ABMs,
a typical social simulation workflow assumes many simultaneous simulation runs,
whereas the fitting step in reconstruction of a synthetic population should normally be performed only once for a  given  dataset.
Therefore,  the  optimal  number  of  nodes  for  the  state-of-the-art  should  be defined by scalability of the synthetic population and CFD codes
(if the latter are of interest for  the  target  GSS  audience).
We  can  always  bypass  the  gap  in  scalability  of  the  synthetic population   and   ABMS   codes
and   reach   full   utilization   of   clusters   by   making   several simultaneous   simulation   runs
(and   treating   simulation   results   in   a   file-avoiding   way).

Unfortunately, our results do not allow to draw solid conclusions about node interconnects
since the benchmarks were done on the testbeds with only one or two nodes.
